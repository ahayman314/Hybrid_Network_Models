{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d093a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca78b",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8512c0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST dataset \n",
    "training_data = torchvision.datasets.MNIST(\"root\", train=True, download=True, transform=ToTensor())\n",
    "testing_data = torchvision.datasets.MNIST(\"root\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860f6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do 0's and 1's\n",
    "#training_data.data = training_data.data[training_data.targets<=1]\n",
    "#training_data.targets = training_data.targets[training_data.targets<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0e3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:\t  torch.Size([60000, 28, 28])\n",
      "Testing data size:\t  torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Display information on datasets\n",
    "print(\"Training data size:\\t \", training_data.data.size())\n",
    "print(\"Testing data size:\\t \", testing_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3f747e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample\n",
    "plt.imshow(training_data.data[0])\n",
    "plt.title('%i' % training_data.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43abb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare subset of data and shuffle\n",
    "train_dataloader = DataLoader(training_data, batch_size = 1000, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc4536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Showcase dataloader information\n",
    "for batch, (images, labels) in enumerate(train_dataloader): \n",
    "    print(batch)\n",
    "    print(images.size())\n",
    "    print(images.view(images.shape[0], -1).size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5534746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.functional.one_hot(training_data.targets)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b0808",
   "metadata": {},
   "source": [
    "## Network Design and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51bc13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1052, -0.3478], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode(inputs, w0, w1, w2, w3):\n",
    "    qml.RX(inputs[0], wires=0)\n",
    "    qml.RX(inputs[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.Rot(*w0, wires=0)\n",
    "    qml.Rot(*w1, wires=1)\n",
    "    qml.RY(w2, wires=0)\n",
    "    qml.RY(w3, wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n",
    "\n",
    "weight_shapes = {\"w0\": 3, \"w1\": 3, \"w2\": 1, \"w3\": 1}\n",
    "qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "qlayer(torch.Tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8ef4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network \n",
    "# Great source: https://pennylane.ai/qml/demos/tutorial_qnn_module_torch.html\n",
    "# Note that in_channels = 1 because the input is a grayscale image\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=784, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.main(x)\n",
    "        x -= x.min(1, keepdim=True)[0]\n",
    "        x/= x.max(1, keepdim=True)[0]\n",
    "        x*= np.pi\n",
    "        x_split = torch.split(x, 2, dim=1)\n",
    "        x_split = list(x_split)\n",
    "        for i in range(len(x_split)): \n",
    "            x_split[i] = qlayer(x_split[i])\n",
    "        x = torch.cat(x_split, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0a602f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (5): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# View neural network \n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd8c84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]         100,480\n",
      "              ReLU-2                  [-1, 128]               0\n",
      "            Linear-3                   [-1, 64]           8,256\n",
      "              ReLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 10]             650\n",
      "        LogSoftmax-6                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(network, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a80de6",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b3e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training routine\n",
    "def train(epochs):\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        accuracy = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (image, labels) in enumerate(train_dataloader):\n",
    "            output = network(image)                           # Find network output\n",
    "            loss = loss_function(output, labels)              # Compute loss\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1]          # Find predicted value\n",
    "            batch_corr = (predicted == labels).sum()          # Find number of correct values\n",
    "            batch_accuracy = (100*batch_corr / len(labels))    \n",
    "            accuracy+=batch_accuracy/60\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()                             # Clear gradients for this training step\n",
    "            loss.backward()                                   # Compute gradients from backpropagation \n",
    "            optimizer.step()                                  # Apply changes from gradients\n",
    "            print(\"Batch accuracy: %.2f\" % batch_accuracy)\n",
    "\n",
    "        print(\"Training accuracy: %.2f \\t Training loss: %.2f \" % (accuracy, running_loss))\n",
    "        epoch_list.append(epoch)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(running_loss)\n",
    "    return epoch_list, accuracy_list, loss_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b08520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [95]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m epochs, accuracy, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [94]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (image, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m                           \u001b[38;5;66;03m# Find network output\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(output, labels)              \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]          \u001b[38;5;66;03m# Find predicted value\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(x_split)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_split)): \n\u001b[1;32m---> 31\u001b[0m     x_split[i] \u001b[38;5;241m=\u001b[39m \u001b[43mqlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_split\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x_split, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\qnn\\torch.py:277\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     reconstructor \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39munbind(inputs):\n\u001b[1;32m--> 277\u001b[0m         reconstructor\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(reconstructor)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# If the input is 1-dimensional, calculate the forward pass as usual\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\qnn\\torch.py:281\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(reconstructor)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# If the input is 1-dimensional, calculate the forward pass as usual\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\qnn\\torch.py:296\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    292\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m    295\u001b[0m }\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\qnode.py:561\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# preprocess the tapes by applying any device-specific transforms\u001b[39;00m\n\u001b[0;32m    559\u001b[0m tapes, processing_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mbatch_transform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape)\n\u001b[1;32m--> 561\u001b[0m res \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m    562\u001b[0m     tapes,\n\u001b[0;32m    563\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    564\u001b[0m     gradient_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn,\n\u001b[0;32m    565\u001b[0m     interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface,\n\u001b[0;32m    566\u001b[0m     gradient_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs,\n\u001b[0;32m    567\u001b[0m     override_shots\u001b[38;5;241m=\u001b[39moverride_shots,\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs,\n\u001b[0;32m    569\u001b[0m )\n\u001b[0;32m    571\u001b[0m res \u001b[38;5;241m=\u001b[39m processing_fn(res)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m override_shots \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# restore the initialization gradient function\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\interfaces\\batch\\__init__.py:321\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_fn \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m interface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcache_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_execute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# the default execution function is batch_execute\u001b[39;00m\n\u001b[0;32m    324\u001b[0m execute_fn \u001b[38;5;241m=\u001b[39m cache_execute(batch_execute, cache, expand_fn\u001b[38;5;241m=\u001b[39mexpand_fn)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\interfaces\\batch\\__init__.py:168\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     res \u001b[38;5;241m=\u001b[39m fn(execution_tapes\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m final_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tapes):\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\interfaces\\batch\\__init__.py:120\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m [expand_fn(tape) \u001b[38;5;28;01mfor\u001b[39;00m tape \u001b[38;5;129;01min\u001b[39;00m tapes]\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn(tapes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Program Files\\Python39\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\_qubit_device.py:278\u001b[0m, in \u001b[0;36mQubitDevice.batch_execute\u001b[1;34m(self, circuits)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m circuit \u001b[38;5;129;01min\u001b[39;00m circuits:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# not start the next computation in the zero state\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 278\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mactive:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:232\u001b[0m, in \u001b[0;36mDefaultQubitTorch.execute\u001b[1;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m params_cuda_device \u001b[38;5;241m!=\u001b[39m specified_device_cuda:\n\u001b[0;32m    225\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    226\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torch_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupon PennyLane device creation does not match the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch device of the gate parameters; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torch_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m             )\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mexecute(circuit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\_qubit_device.py:194\u001b[0m, in \u001b[0;36mQubitDevice.execute\u001b[1;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_validity(circuit\u001b[38;5;241m.\u001b[39moperations, circuit\u001b[38;5;241m.\u001b[39mobservables)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# apply all circuit operations\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(circuit\u001b[38;5;241m.\u001b[39moperations, rotations\u001b[38;5;241m=\u001b[39mcircuit\u001b[38;5;241m.\u001b[39mdiagonalizing_gates, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# generate computational basis samples\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshots \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39mis_sampled:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:216\u001b[0m, in \u001b[0;36mDefaultQubit.apply\u001b[1;34m(self, operations, rotations, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_basis_state(operation\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;241m0\u001b[39m], operation\u001b[38;5;241m.\u001b[39mwires)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# store the pre-rotated state\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_rotated_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:247\u001b[0m, in \u001b[0;36mDefaultQubit._apply_operation\u001b[1;34m(self, state, operation)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_diagonal_unitary(state, matrix, wires)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(wires) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;66;03m# Einsum is faster for small gates\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_unitary_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwires\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_unitary(state, matrix, wires)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:752\u001b[0m, in \u001b[0;36mDefaultQubit._apply_unitary_einsum\u001b[1;34m(self, state, mat, wires)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# We now put together the indices in the notation numpy's einsum requires\u001b[39;00m\n\u001b[0;32m    750\u001b[0m einsum_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00maffected_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_state_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\functional.py:327\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "epochs, accuracy, loss = train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4cf3464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x245806a7f40>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b76ece",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(): \n",
    "    accuracy = 0\n",
    "    with torch.no_grad(): \n",
    "        for image, labels in enumerate(test_dataloader): \n",
    "            output = network(image)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            accuracy += (100*(predicted == labels).sum() / len(labels))\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde859be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11a1f8",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd069e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58fbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e393f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba912c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = [] \n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e00b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "773ec097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\AppData\\Local\\Temp\\ipykernel_520\\161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3063, Accuracy: 1377/10000 (14%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312027\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m train_counter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     17\u001b[0m     (batch_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m+\u001b[39m ((epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)))\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/results/model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(optimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/optimizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/model.pth'"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
