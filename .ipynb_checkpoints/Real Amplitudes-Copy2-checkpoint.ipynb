{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d093a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Andrew Hayman\\PycharmProjects\\Hybrid_Network_Models\\venv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9adc3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29257636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.FloatTensor([0,0,1])\n",
    "X_train = X_train.to(device)\n",
    "X_train.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590ab84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca78b",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8512c0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST dataset \n",
    "training_data = torchvision.datasets.MNIST(\"root\", train=True, download=True, transform=ToTensor())\n",
    "testing_data = torchvision.datasets.MNIST(\"root\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860f6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do 0's and 1's\n",
    "#training_data.data = training_data.data[training_data.targets<=1]\n",
    "#training_data.targets = training_data.targets[training_data.targets<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0e3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:\t  torch.Size([60000, 28, 28])\n",
      "Testing data size:\t  torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Display information on datasets\n",
    "print(\"Training data size:\\t \", training_data.data.size())\n",
    "print(\"Testing data size:\\t \", testing_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3f747e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample\n",
    "plt.imshow(training_data.data[0])\n",
    "plt.title('%i' % training_data.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43abb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare subset of data and shuffle\n",
    "train_dataloader = DataLoader(training_data, batch_size = 1000, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc4536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Showcase dataloader information\n",
    "for batch, (images, labels) in enumerate(train_dataloader): \n",
    "    print(batch)\n",
    "    print(images.size())\n",
    "    print(images.view(images.shape[0], -1).size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5534746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.functional.one_hot(training_data.targets)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b0808",
   "metadata": {},
   "source": [
    "## Network Design and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bc13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3487, -0.9325,  0.7871], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 3\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_3_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2))\n",
    "\n",
    "weight_shapes = {\"weights\": 3}\n",
    "qlayer_3_amplitudes = qml.qnn.TorchLayer(qnode_3_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_3_amplitudes(torch.Tensor([1,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc57fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9017, -0.1478, -0.1700, -0.5965], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_4_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[1, 3])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "    qml.RY(weights[3], wires=3)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3))\n",
    "\n",
    "weight_shapes = {\"weights\": 4}\n",
    "qlayer_4_amplitudes = qml.qnn.TorchLayer(qnode_4_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_4_amplitudes(torch.Tensor([1,3,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d084442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5432, -0.1228,  0.0314, -0.1574, -0.9339],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 5\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_5_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "    qml.Hadamard(wires=4)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    qml.RY(inputs[4], wires=4)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "    qml.CNOT(wires=[0, 4])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[1, 3])\n",
    "    qml.CNOT(wires=[1, 4])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[2, 4])\n",
    "    qml.CNOT(wires=[3, 4])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "    qml.RY(weights[3], wires=3)\n",
    "    qml.RY(weights[4], wires=4)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3)), qml.expval(qml.PauliZ(4))\n",
    "\n",
    "weight_shapes = {\"weights\": 5}\n",
    "qlayer_5_amplitudes = qml.qnn.TorchLayer(qnode_5_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_5_amplitudes(torch.Tensor([1,3,3,5,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8ef4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network \n",
    "# Great source: https://pennylane.ai/qml/demos/tutorial_qnn_module_torch.html\n",
    "# Note that in_channels = 1 because the input is a grayscale image\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=784, out_features=1000),\n",
    "            nn.Linear(in_features=1000, out_features=100),\n",
    "            nn.Linear(in_features=100, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Sequential(nn.Linear(in_features=10, out_features=10),\n",
    "                                  nn.LogSoftmax(dim=1))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.main(x)\n",
    "        x_split = torch.split(x, 5, dim=1)\n",
    "        x_split = list(x_split)\n",
    "        x_split[0] = qlayer_5_amplitudes(x_split[0])\n",
    "        x_split[1] = qlayer_5_amplitudes(x_split[1])\n",
    "        x = torch.cat(x_split, dim=1)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a602f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
      "    (1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
       "    (1): Linear(in_features=1000, out_features=100, bias=True)\n",
       "    (2): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View neural network \n",
    "network = Net()\n",
    "print(network)\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd9b7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\cuda\\memory.py:384: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23068672"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del(optimizer)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd8c84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#summary(network, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a80de6",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b3e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training routine\n",
    "import time\n",
    "\n",
    "def train(epochs):\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        accuracy = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (image, labels) in enumerate(train_dataloader):\n",
    "            start = time.time()\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = network(image)                           # Find network output\n",
    "            loss = loss_function(output, labels)              # Compute loss\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1]          # Find predicted value\n",
    "            batch_corr = (predicted == labels).sum()          # Find number of correct values\n",
    "            batch_accuracy = (100*batch_corr / len(labels))    \n",
    "            accuracy+=batch_accuracy/60\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()                             # Clear gradients for this training step\n",
    "            loss.backward()                                   # Compute gradients from backpropagation \n",
    "            optimizer.step()                                  # Apply changes from gradients\n",
    "            end = time.time()\n",
    "            print(\"Batch accuracy: %.2f Time: %.2f\" % (batch_accuracy, (end-start))\n",
    "\n",
    "        print(\"Training accuracy: %.2f \\t Training loss: %.2f \" % (accuracy, running_loss))\n",
    "        epoch_list.append(epoch)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(running_loss)\n",
    "    return epoch_list, accuracy_list, loss_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc77c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 8.40\n",
      "Batch accuracy: 17.80\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "epochs, accuracy, loss = train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4cf3464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2972882bd30>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b76ece",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(): \n",
    "    accuracy = 0\n",
    "    with torch.no_grad(): \n",
    "        for image, labels in enumerate(test_dataloader): \n",
    "            output = network(image)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            accuracy += (100*(predicted == labels).sum() / len(labels))\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde859be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11a1f8",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd069e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58fbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e393f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba912c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = [] \n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e00b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "773ec097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\AppData\\Local\\Temp\\ipykernel_520\\161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3063, Accuracy: 1377/10000 (14%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312027\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m train_counter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     17\u001b[0m     (batch_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m+\u001b[39m ((epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)))\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/results/model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(optimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/optimizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/model.pth'"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
