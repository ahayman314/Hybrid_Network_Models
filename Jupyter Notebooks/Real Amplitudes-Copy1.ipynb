{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d093a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca78b",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8512c0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST dataset \n",
    "training_data = torchvision.datasets.MNIST(\"root\", train=True, download=True, transform=ToTensor())\n",
    "testing_data = torchvision.datasets.MNIST(\"root\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do 0's and 1's\n",
    "#training_data.data = training_data.data[training_data.targets<=1]\n",
    "#training_data.targets = training_data.targets[training_data.targets<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information on datasets\n",
    "print(\"Training data size:\\t \", training_data.data.size())\n",
    "print(\"Testing data size:\\t \", testing_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample\n",
    "plt.imshow(training_data.data[0])\n",
    "plt.title('%i' % training_data.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43abb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare subset of data and shuffle\n",
    "train_dataloader = DataLoader(training_data, batch_size = 1000, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc4536d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Showcase dataloader information\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m): \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Showcase dataloader information\n",
    "for batch, (images, labels) in enumerate(train_dataloader): \n",
    "    print(batch)\n",
    "    print(images.size())\n",
    "    print(images.view(images.shape[0], -1).size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5534746",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.nn.functional.one_hot(training_data.targets)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b0808",
   "metadata": {},
   "source": [
    "## Network Design and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51bc13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.1367, 0.3398], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pennylane_qulacs.qulacs_device import QulacsDevice\n",
    "\n",
    "n_qubits = 3\n",
    "#dev = qml.device(\"qulacs.simulator\", wires=n_qubits)\n",
    "dev = QulacsDevice(wires=n_qubits, shots=1024)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_3_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2))\n",
    "\n",
    "weight_shapes = {\"weights\": 3}\n",
    "qlayer_3_amplitudes = qml.qnn.TorchLayer(qnode_3_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_3_amplitudes(torch.Tensor([1,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380f7be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2118, -0.5441,  0.1060, -0.1098], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_4_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[1, 3])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "    qml.RY(weights[3], wires=3)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3))\n",
    "\n",
    "weight_shapes = {\"weights\": 4}\n",
    "qlayer_4_amplitudes = qml.qnn.TorchLayer(qnode_4_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_4_amplitudes(torch.Tensor([1,3,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71d30dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5766, -0.1177, -0.0610, -0.1184,  0.7332],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 5\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode_5_amplitudes(inputs, weights):\n",
    "    \n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.Hadamard(wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "    qml.Hadamard(wires=4)\n",
    "    \n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    qml.RY(inputs[4], wires=4)\n",
    "    \n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[0, 2])\n",
    "    qml.CNOT(wires=[0, 3])\n",
    "    qml.CNOT(wires=[0, 4])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[1, 3])\n",
    "    qml.CNOT(wires=[1, 4])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[2, 4])\n",
    "    qml.CNOT(wires=[3, 4])\n",
    "    \n",
    "    qml.RY(weights[0], wires=0)\n",
    "    qml.RY(weights[1], wires=1)\n",
    "    qml.RY(weights[2], wires=2)\n",
    "    qml.RY(weights[3], wires=3)\n",
    "    qml.RY(weights[4], wires=4)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2)), qml.expval(qml.PauliZ(3)), qml.expval(qml.PauliZ(4))\n",
    "\n",
    "weight_shapes = {\"weights\": 5}\n",
    "qlayer_5_amplitudes = qml.qnn.TorchLayer(qnode_5_amplitudes, weight_shapes)\n",
    "\n",
    "qlayer_5_amplitudes(torch.Tensor([1,3,3,5,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ef4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network \n",
    "# Great source: https://pennylane.ai/qml/demos/tutorial_qnn_module_torch.html\n",
    "# Note that in_channels = 1 because the input is a grayscale image\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=784, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Sequential(nn.Linear(in_features=360, out_features=64),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(in_features=64, out_features=10),\n",
    "                                    nn.LogSoftmax(dim=1))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.main(x)\n",
    "        x_split = torch.split(x, 1, dim=1)\n",
    "        x_split = list(x_split)\n",
    "\n",
    "        x_out = []\n",
    "        \n",
    "        i=0\n",
    "        for combo in combinations(list(range(0,10)), 3):\n",
    "            i+=1\n",
    "            print(\"combo\", i)\n",
    "            x_temp = torch.cat([x_split[combo[0]], x_split[combo[1]], x_split[combo[2]]], dim=1)\n",
    "            x_temp = qlayer_3_amplitudes(x_temp)\n",
    "            x_out.append(x_temp)\n",
    "        \n",
    "        x = torch.cat(x_out, dim=1)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a602f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (3): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# View neural network \n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd8c84df",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     x_split \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(x, \u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m     x_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(x_split)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1120\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(network, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a80de6",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b3e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training routine\n",
    "def train(epochs):\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        accuracy = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (image, labels) in enumerate(train_dataloader):\n",
    "            output = network(image)                           # Find network output\n",
    "            loss = loss_function(output, labels)              # Compute loss\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1]          # Find predicted value\n",
    "            batch_corr = (predicted == labels).sum()          # Find number of correct values\n",
    "            batch_accuracy = (100*batch_corr / len(labels))    \n",
    "            accuracy+=batch_accuracy/60\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()                             # Clear gradients for this training step\n",
    "            loss.backward()                                   # Compute gradients from backpropagation \n",
    "            optimizer.step()                                  # Apply changes from gradients\n",
    "            print(\"Batch accuracy: %.2f\" % batch_accuracy)\n",
    "\n",
    "        print(\"Training accuracy: %.2f \\t Training loss: %.2f \" % (accuracy, running_loss))\n",
    "        epoch_list.append(epoch)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(running_loss)\n",
    "    return epoch_list, accuracy_list, loss_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc77c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combo 1\n",
      "combo 2\n",
      "combo 3\n",
      "combo 4\n",
      "combo 5\n",
      "combo 6\n",
      "combo 7\n",
      "combo 8\n",
      "combo 9\n",
      "combo 10\n",
      "combo 11\n",
      "combo 12\n",
      "combo 13\n",
      "combo 14\n",
      "combo 15\n",
      "combo 16\n",
      "combo 17\n",
      "combo 18\n",
      "combo 19\n",
      "combo 20\n",
      "combo 21\n",
      "combo 22\n",
      "combo 23\n",
      "combo 24\n",
      "combo 25\n",
      "combo 26\n",
      "combo 27\n",
      "combo 28\n",
      "combo 29\n",
      "combo 30\n",
      "combo 31\n",
      "combo 32\n",
      "combo 33\n",
      "combo 34\n",
      "combo 35\n",
      "combo 36\n",
      "combo 37\n",
      "combo 38\n",
      "combo 39\n",
      "combo 40\n",
      "combo 41\n",
      "combo 42\n",
      "combo 43\n",
      "combo 44\n",
      "combo 45\n",
      "combo 46\n",
      "combo 47\n",
      "combo 48\n",
      "combo 49\n",
      "combo 50\n",
      "combo 51\n",
      "combo 52\n",
      "combo 53\n",
      "combo 54\n",
      "combo 55\n",
      "combo 56\n",
      "combo 57\n",
      "combo 58\n",
      "combo 59\n",
      "combo 60\n",
      "combo 61\n",
      "combo 62\n",
      "combo 63\n",
      "combo 64\n",
      "combo 65\n",
      "combo 66\n",
      "combo 67\n",
      "combo 68\n",
      "combo 69\n",
      "combo 70\n",
      "combo 71\n",
      "combo 72\n",
      "combo 73\n",
      "combo 74\n",
      "combo 75\n",
      "combo 76\n",
      "combo 77\n",
      "combo 78\n",
      "combo 79\n",
      "combo 80\n",
      "combo 81\n",
      "combo 82\n",
      "combo 83\n",
      "combo 84\n",
      "combo 85\n",
      "combo 86\n",
      "combo 87\n",
      "combo 88\n",
      "combo 89\n",
      "combo 90\n",
      "combo 91\n",
      "combo 92\n",
      "combo 93\n",
      "combo 94\n",
      "combo 95\n",
      "combo 96\n",
      "combo 97\n",
      "combo 98\n",
      "combo 99\n",
      "combo 100\n",
      "combo 101\n",
      "combo 102\n",
      "combo 103\n",
      "combo 104\n",
      "combo 105\n",
      "combo 106\n",
      "combo 107\n",
      "combo 108\n",
      "combo 109\n",
      "combo 110\n",
      "combo 111\n",
      "combo 112\n",
      "combo 113\n",
      "combo 114\n",
      "combo 115\n",
      "combo 116\n",
      "combo 117\n",
      "combo 118\n",
      "combo 119\n",
      "combo 120\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "epochs, accuracy, loss = train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4cf3464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2972882bd30>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c48639",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b76ece",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadf05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(): \n",
    "    accuracy = 0\n",
    "    with torch.no_grad(): \n",
    "        for image, labels in enumerate(test_dataloader): \n",
    "            output = network(image)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            accuracy += (100*(predicted == labels).sum() / len(labels))\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde859be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11a1f8",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd069e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58fbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e393f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba912c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = [] \n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e00b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "773ec097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\AppData\\Local\\Temp\\ipykernel_520\\161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3063, Accuracy: 1377/10000 (14%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312027\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m train_counter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     17\u001b[0m     (batch_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m+\u001b[39m ((epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)))\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/results/model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(optimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/optimizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/model.pth'"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702ff91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
