{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d093a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca78b",
   "metadata": {},
   "source": [
    "# MNIST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8512c0",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4405ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from MNIST dataset \n",
    "training_data = torchvision.datasets.MNIST(\"root\", train=True, download=True, transform=ToTensor())\n",
    "testing_data = torchvision.datasets.MNIST(\"root\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860f6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do 0's and 1's\n",
    "#training_data.data = training_data.data[training_data.targets<=1]\n",
    "#training_data.targets = training_data.targets[training_data.targets<=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0e3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:\t  torch.Size([60000, 28, 28])\n",
      "Testing data size:\t  torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Display information on datasets\n",
    "print(\"Training data size:\\t \", training_data.data.size())\n",
    "print(\"Testing data size:\\t \", testing_data.data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3f747e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample\n",
    "plt.imshow(training_data.data[0])\n",
    "plt.title('%i' % training_data.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43abb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare subset of data and shuffle\n",
    "train_dataloader = DataLoader(training_data, batch_size = 1000, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = 100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc4536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1000, 1, 28, 28])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# Showcase dataloader information\n",
    "for batch, (images, labels) in enumerate(train_dataloader): \n",
    "    print(batch)\n",
    "    print(images.size())\n",
    "    print(images.view(images.shape[0], -1).size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5534746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test = torch.nn.functional.one_hot(training_data.targets)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2b0808",
   "metadata": {},
   "source": [
    "## Network Design and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51bc13b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4206,  0.9028], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def qnode(inputs, w0, w1, w2, w3):\n",
    "    qml.RX(inputs[0], wires=0)\n",
    "    qml.RX(inputs[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.Rot(*w0, wires=0)\n",
    "    qml.Rot(*w1, wires=1)\n",
    "    qml.RY(w2, wires=0)\n",
    "    qml.RY(w3, wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))\n",
    "\n",
    "weight_shapes = {\"w0\": 3, \"w1\": 3, \"w2\": 1, \"w3\": 1}\n",
    "qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n",
    "\n",
    "qlayer(torch.Tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8ef4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network \n",
    "# Great source: https://pennylane.ai/qml/demos/tutorial_qnn_module_torch.html\n",
    "# Note that in_channels = 1 because the input is a grayscale image\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=784, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Sequential(nn.Linear(in_features=10, out_features=10),\n",
    "                                  nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.main(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0a602f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): LogSoftmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# View neural network \n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd8c84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 128]         100,480\n",
      "              ReLU-2                  [-1, 128]               0\n",
      "            Linear-3                   [-1, 64]           8,256\n",
      "              ReLU-4                   [-1, 64]               0\n",
      "            Linear-5                   [-1, 10]             650\n",
      "            Linear-6                   [-1, 10]             110\n",
      "        LogSoftmax-7                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 109,496\n",
      "Trainable params: 109,496\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(network, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a80de6",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b3e5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training routine\n",
    "def train(epochs):\n",
    "    epoch_list = []\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        accuracy = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch_idx, (image, labels) in enumerate(train_dataloader):\n",
    "            output = network(image)                           # Find network output\n",
    "            loss = loss_function(output, labels)              # Compute loss\n",
    "\n",
    "            predicted = torch.max(output.data, 1)[1]          # Find predicted value\n",
    "            batch_corr = (predicted == labels).sum()          # Find number of correct values\n",
    "            batch_accuracy = (100*batch_corr / len(labels))    \n",
    "            accuracy+=batch_accuracy/60\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()                             # Clear gradients for this training step\n",
    "            loss.backward()                                   # Compute gradients from backpropagation \n",
    "            optimizer.step()                                  # Apply changes from gradients\n",
    "            print(\"Batch accuracy: %.2f\" % batch_accuracy)\n",
    "\n",
    "        print(\"Training accuracy: %.2f \\t Training loss: %.2f \" % (accuracy, running_loss))\n",
    "        epoch_list.append(epoch)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(running_loss)\n",
    "    return epoch_list, accuracy_list, loss_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dcf9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 9.40\n",
      "Batch accuracy: 11.60\n",
      "Batch accuracy: 42.00\n",
      "Batch accuracy: 40.90\n",
      "Batch accuracy: 53.20\n",
      "Batch accuracy: 54.20\n",
      "Batch accuracy: 56.50\n",
      "Batch accuracy: 62.70\n",
      "Batch accuracy: 67.70\n",
      "Batch accuracy: 73.90\n",
      "Batch accuracy: 69.10\n",
      "Batch accuracy: 77.40\n",
      "Batch accuracy: 75.90\n",
      "Batch accuracy: 80.00\n",
      "Batch accuracy: 78.50\n",
      "Batch accuracy: 77.40\n",
      "Batch accuracy: 84.20\n",
      "Batch accuracy: 83.80\n",
      "Batch accuracy: 83.70\n",
      "Batch accuracy: 87.30\n",
      "Batch accuracy: 85.30\n",
      "Batch accuracy: 87.40\n",
      "Batch accuracy: 86.70\n",
      "Batch accuracy: 87.20\n",
      "Batch accuracy: 88.20\n",
      "Batch accuracy: 89.20\n",
      "Batch accuracy: 87.60\n",
      "Batch accuracy: 90.70\n",
      "Batch accuracy: 88.10\n",
      "Batch accuracy: 88.70\n",
      "Batch accuracy: 91.20\n",
      "Batch accuracy: 91.20\n",
      "Batch accuracy: 90.90\n",
      "Batch accuracy: 90.40\n",
      "Batch accuracy: 90.50\n",
      "Batch accuracy: 92.20\n",
      "Batch accuracy: 92.40\n",
      "Batch accuracy: 91.10\n",
      "Batch accuracy: 91.40\n",
      "Batch accuracy: 92.90\n",
      "Batch accuracy: 92.10\n",
      "Batch accuracy: 92.10\n",
      "Batch accuracy: 93.30\n",
      "Batch accuracy: 93.20\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 92.90\n",
      "Batch accuracy: 92.30\n",
      "Batch accuracy: 92.50\n",
      "Batch accuracy: 92.60\n",
      "Batch accuracy: 93.90\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 92.30\n",
      "Batch accuracy: 93.10\n",
      "Batch accuracy: 93.10\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 94.00\n",
      "Batch accuracy: 94.20\n",
      "Batch accuracy: 94.10\n",
      "Batch accuracy: 93.80\n",
      "Batch accuracy: 93.60\n",
      "Training accuracy: 82.12 \t Training loss: 32.58 \n",
      "Batch accuracy: 93.60\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 95.00\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 94.60\n",
      "Batch accuracy: 94.20\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 96.10\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 95.30\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 94.90\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 95.00\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 94.90\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 94.10\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 95.00\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 96.60\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 95.30\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 95.60\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.10\n",
      "Batch accuracy: 96.00\n",
      "Batch accuracy: 95.80\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 96.60\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 97.50\n",
      "Training accuracy: 95.56 \t Training loss: 8.76 \n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 95.80\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 96.00\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 95.90\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.90\n",
      "Training accuracy: 97.02 \t Training loss: 5.87 \n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.70\n",
      "Training accuracy: 97.77 \t Training loss: 4.32 \n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.10\n",
      "Training accuracy: 98.18 \t Training loss: 3.45 \n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.70\n",
      "Training accuracy: 98.51 \t Training loss: 2.77 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Training accuracy: 98.87 \t Training loss: 2.14 \n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.70\n",
      "Training accuracy: 98.99 \t Training loss: 1.90 \n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.90\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.80\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Training accuracy: 99.10 \t Training loss: 1.57 \n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.80\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.80\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.30\n",
      "Training accuracy: 99.12 \t Training loss: 1.56 \n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "epochs, accuracy, loss = train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b08520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 9.20\n",
      "Batch accuracy: 25.30\n",
      "Batch accuracy: 46.70\n",
      "Batch accuracy: 44.90\n",
      "Batch accuracy: 45.20\n",
      "Batch accuracy: 55.60\n",
      "Batch accuracy: 58.70\n",
      "Batch accuracy: 62.70\n",
      "Batch accuracy: 67.40\n",
      "Batch accuracy: 69.30\n",
      "Batch accuracy: 73.30\n",
      "Batch accuracy: 74.70\n",
      "Batch accuracy: 74.90\n",
      "Batch accuracy: 78.30\n",
      "Batch accuracy: 79.50\n",
      "Batch accuracy: 78.70\n",
      "Batch accuracy: 80.30\n",
      "Batch accuracy: 84.90\n",
      "Batch accuracy: 81.50\n",
      "Batch accuracy: 82.90\n",
      "Batch accuracy: 86.10\n",
      "Batch accuracy: 85.40\n",
      "Batch accuracy: 85.60\n",
      "Batch accuracy: 87.20\n",
      "Batch accuracy: 86.60\n",
      "Batch accuracy: 86.60\n",
      "Batch accuracy: 87.60\n",
      "Batch accuracy: 88.00\n",
      "Batch accuracy: 88.40\n",
      "Batch accuracy: 89.40\n",
      "Batch accuracy: 89.00\n",
      "Batch accuracy: 90.50\n",
      "Batch accuracy: 89.20\n",
      "Batch accuracy: 89.60\n",
      "Batch accuracy: 91.10\n",
      "Batch accuracy: 89.00\n",
      "Batch accuracy: 90.80\n",
      "Batch accuracy: 88.30\n",
      "Batch accuracy: 89.20\n",
      "Batch accuracy: 91.10\n",
      "Batch accuracy: 90.10\n",
      "Batch accuracy: 90.90\n",
      "Batch accuracy: 89.80\n",
      "Batch accuracy: 91.30\n",
      "Batch accuracy: 91.30\n",
      "Batch accuracy: 93.10\n",
      "Batch accuracy: 93.00\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 90.60\n",
      "Batch accuracy: 91.90\n",
      "Batch accuracy: 91.50\n",
      "Batch accuracy: 91.80\n",
      "Batch accuracy: 93.10\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 93.80\n",
      "Batch accuracy: 93.10\n",
      "Batch accuracy: 93.20\n",
      "Batch accuracy: 93.80\n",
      "Batch accuracy: 92.50\n",
      "Batch accuracy: 92.90\n",
      "Training accuracy: 81.46 \t Training loss: 35.20 \n",
      "Batch accuracy: 94.40\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 93.20\n",
      "Batch accuracy: 94.40\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 93.60\n",
      "Batch accuracy: 94.00\n",
      "Batch accuracy: 93.40\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 94.30\n",
      "Batch accuracy: 94.20\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 93.80\n",
      "Batch accuracy: 94.10\n",
      "Batch accuracy: 95.10\n",
      "Batch accuracy: 94.30\n",
      "Batch accuracy: 94.60\n",
      "Batch accuracy: 93.90\n",
      "Batch accuracy: 94.60\n",
      "Batch accuracy: 93.40\n",
      "Batch accuracy: 93.70\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 94.30\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 93.20\n",
      "Batch accuracy: 94.60\n",
      "Batch accuracy: 94.00\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 94.40\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 94.50\n",
      "Batch accuracy: 95.30\n",
      "Batch accuracy: 94.60\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 95.80\n",
      "Batch accuracy: 95.10\n",
      "Batch accuracy: 95.00\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 93.60\n",
      "Batch accuracy: 95.00\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 95.50\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 94.80\n",
      "Batch accuracy: 95.80\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 94.70\n",
      "Batch accuracy: 96.10\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 95.30\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.70\n",
      "Training accuracy: 94.78 \t Training loss: 10.36 \n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 96.00\n",
      "Batch accuracy: 96.60\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 95.30\n",
      "Batch accuracy: 96.80\n",
      "Batch accuracy: 94.90\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 95.40\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 96.60\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 95.80\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 96.10\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 96.00\n",
      "Batch accuracy: 95.70\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.60\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 95.20\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 96.10\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 96.00\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 96.30\n",
      "Batch accuracy: 96.80\n",
      "Training accuracy: 96.51 \t Training loss: 6.88 \n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 95.90\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 96.20\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 96.40\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 96.70\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 97.70\n",
      "Training accuracy: 97.49 \t Training loss: 5.00 \n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 97.00\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 96.50\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.10\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 96.90\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Training accuracy: 98.00 \t Training loss: 3.93 \n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 97.30\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 97.50\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 97.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 97.40\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.30\n",
      "Training accuracy: 98.40 \t Training loss: 3.09 \n",
      "Batch accuracy: 98.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 97.20\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 97.90\n",
      "Batch accuracy: 97.80\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 97.60\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.30\n",
      "Training accuracy: 98.52 \t Training loss: 2.72 \n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.90\n",
      "Training accuracy: 98.95 \t Training loss: 1.95 \n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.80\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.00\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.30\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.70\n",
      "Training accuracy: 99.00 \t Training loss: 1.82 \n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.50\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 98.10\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.00\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.70\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.50\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 98.90\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.60\n",
      "Batch accuracy: 99.30\n",
      "Batch accuracy: 99.40\n",
      "Batch accuracy: 99.20\n",
      "Batch accuracy: 99.10\n",
      "Batch accuracy: 98.70\n",
      "Batch accuracy: 98.40\n",
      "Batch accuracy: 98.80\n",
      "Batch accuracy: 99.20\n",
      "Training accuracy: 99.21 \t Training loss: 1.52 \n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "epochs, accuracy, loss = train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d36f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b76ece",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(): \n",
    "    accuracy = 0\n",
    "    with torch.no_grad(): \n",
    "        for image, labels in enumerate(test_dataloader): \n",
    "            output = network(image)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            accuracy += (100*(predicted == labels).sum() / len(labels))\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c33ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119767ea",
   "metadata": {},
   "source": [
    "# Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db361f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152654c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c00f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5547b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba912c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = [] \n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abde6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 10\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), '/results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), '/results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e00b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "773ec097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Hayman\\AppData\\Local\\Temp\\ipykernel_520\\161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3063, Accuracy: 1377/10000 (14%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312027\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m train_counter\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     17\u001b[0m     (batch_idx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m64\u001b[39m) \u001b[38;5;241m+\u001b[39m ((epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)))\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/results/model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(optimizer\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/results/optimizer.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Hybrid_Network_Models\\venv\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/model.pth'"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2702ff91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "print(torch.split(a,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
